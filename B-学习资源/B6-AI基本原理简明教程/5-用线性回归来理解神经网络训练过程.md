下面我们举一个简单的线性回归的例子来说明实际的反向传播和梯度下降的过程。完全看懂此文后，会对理解后续的文章有很大的帮助。

为什么要用线性回归举例呢？因为$y = wx+b$ （其中，y,w,x,b都是标量）这个函数的形式和神经网络中的$Y = WX + B$（其中，Y,W,X,B等都是矩阵）非常近似，可以起到用简单的原理理解复杂的事情的作用。

# 创造训练数据

让我们先自力更生创造一些模拟数据：

```Python
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def create_sample_data(m):
    # check if saved before
    Xfile = Path("XData.npy")
    Yfile = Path("YData.npy")
    # load data from file
    if Xfile.exists() & Yfile.exists():
        X = np.load(Xfile)
        Y = np.load(Yfile)
    else: # generate new data
        X = np.random.random(m)
        # create some offset as noise to simulate real data
        noise = np.random.normal(0,0.1,X.shape)
        # genarate Y data
        W = 2
        B = 3
        Y = X * W + B + noise
        np.save("XData.npy", X)
        np.save("YData.npy", Y)
    return X, Y
```

得到100个数据点如下：

<img src=".\Images\5-sample1.png" width="400">  

好了，模拟数据制作好了，目前X是一个100个元素的集合，里面有0~100之间的随机x点，Y是一个100个元素的集合，里面有对应到每个x上的$y=2x+3$的值，然后再加一个或正或负的上下偏移作为噪音，来满足对实际数据的模拟效果（因为大部分真实世界的生产数据从来都不是精确的，精确只存在于数学领域）。

现在我们要忘记这些模拟数据（样本值）是如何制作出来的，也就是要忘记W,B的值。我们就假设这是实际应用中收集到的模拟数据，但是我们并不知道它的原始函数是什么参数，只知道是公式$y = wx + b$，我们的任务就是要根据这些样本值，通过神经网络训练的方式，得到w和b的值。注意这里x和y是样本的输入和输出，不是目标变量，这一点和常见的初等数学题不一样，要及时转变概念。

最终，样本数据的样子是：

$$
\begin{pmatrix}
x_1\\
x_2\\
\dots\\
x_m\\
\end{pmatrix}
,
\begin{pmatrix}
y_1\\
y_2\\
\dots\\
y_m\\
\end{pmatrix}
$$

其中，x就是上图中蓝色点的横坐标值，y是纵坐标值。

# 训练方式的选择

我们是首次尝试建立神经网络，先搞一个最简单的单点神经元：

<img src=".\Images\5-SingleCell.jpg" width="600">  

所有的样本数据x，乘以相同的w值后相加，再加上偏移b，输出z。

接下来，我们会用两种方式来训练神经网络（神经元）：

1. 随机梯度下降SDG (Stochastic Gradient Descent)：每次迭代只使用一个样本进行训练
2. 批量梯度下降BDG (Batch Gradient Descent)：把所有样本整批的输入网络进行训练

Pseudo code如下：

第一种方式：逐个样本训练即随机梯度下降
```Python
repeat:
    for 每个样本x,y:
        标量计算得到z的单值 z = wx+b
        计算w的梯度
        计算b的梯度
        更新w,b的值
        计算本次损失
        与上一次的损失值比较，足够小的话就停止训练
    end for
until stop condition
```

第二种方式：批量样本训练即批量梯度下降

```Python
repeat:
    矩阵前向计算得到Z值 = wX+b（其中X是所有样本的一个数组/集合）
    计算w的梯度
    计算b的梯度
    更新w,b的值
    计算本批损失
    与上一批的损失值比较，足够小的话就停止训练
until stop condition
```

我们看完它们的训练结构后再来比较两者的好坏。

# 使用第一种方式训练 - 独立样本

## 定义神经网络结构

对于简单的线性回归问题，我们使用单层网络单个神经元就足够了。而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解。

```Python
def forward_calculation(w,b,X):
    z = w * x + b
    return z
```

其中，由于X是一组数据（100个），所以它是一个矢量，或者理解为一维数组。w和b都是一个标量，Z的计算结果也是一个矢量，尺寸和X一样。

上面的写法，实际上是每次迭代都用所有的样本做训练，因为输入是X，是所有样本的集合。还有另外一种做法，就是每次训练，只用一个训练样本，那么就需要在主循环中进行调度，一次使用一个样本。

## 定义代价函数


我们用传统的均方差函数: $loss = \frac{1}{2}(Z-Y)^2$，其中，Z是每一次迭代的预测输出，Y是样本标签数据。我们使用所有样本参与训练，因此损失函数实际为：

$$loss = \frac{1}{2m}\sum_{i=1}^{m}(Z_i - Y_i) ^ 2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

由于loss是所有样本的集合，我们先对其中的所有值求总和，样本数量是m，然后除以m来求一个平均值。

<img src=".\Images\5-mse.png" width="500">  

假设我们计算出初步的结果是红色虚线所示，这条直线是否合适呢？我们来计算一下图中每个点到这条直线的距离（黄色线），把这些距离的值都加起来（都是正数，不存在互相抵消的问题）成为loss，然后想办法不断改变红色直线的角度和位置，让loss最小，就意味着整体偏差最小，那么最终的那条红色直线就是我们要的结果。


下面是Python的code，用于计算损失：

```Python
# w:weight, Y:sample data, m:count of sample
def check_diff(w, b, X, Y, m):
    Z = w * X + b
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/m
    return loss
```

我们计算这个loss值的目的是计算前后两次迭代的loss值差异，当足够小时，就结束训练。

## 定义针对w和b的梯度函数

### 求w的梯度
因为：

$$z = wx+b$$

$$loss = \frac{1}{2}(z-y)^2$$

所以我们用loss的值作为基准，去求w对它的影响，也就是loss对w的偏导数：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{z}}*\frac{\partial{z}}{\partial{w}}
$$

其中：

$$
\frac{\partial{loss}}{\partial{z}} = \frac{\partial{(\frac{1}{2}(z-y)^2)}}{\partial{z}} = z-y
$$

而：

$$
\frac{\partial{z}}{\partial{w}} = \frac{\partial{}}{\partial{w}}(wx+b) = x
$$

所以：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{z}}*\frac{\partial{z}}{\partial{w}} = (z-y)x
$$

### 求b的梯度

所以我们用loss的值作为基准，去求w对它的影响，也就是loss对w的偏导数：

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z}}*\frac{\partial{z}}{\partial{b}}
$$

其中第一项前面算w的时候已经有了，而：

$$
\frac{\partial{z}}{\partial{b}} = \frac{\partial{(wx+b)}}{\partial{b}} = 1
$$

所以：

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{z}}*\frac{\partial{z}}{\partial{b}} = z-y
$$

```Python
# z:predication value, y:sample data label, x:sample data
def dJwb(z,y,x):
    v = z - y
    db = v
    dw = v * x
    return dw, db
```

## 每次迭代后更新w,b的值

```Python
def update_weights(w, b, dw, db, eta):
    w = w - eta*dw
    b = b - eta*db
    return w,b
```
eta在本程序中恒等于0.01，这是随机梯度下降法。也可以在迭代到一定次数后，把eta的值逐步减小，变成0.001，这样会形成开始时大步前进，到后面时小步快跑的局面，利于训练准确度提高。

## 初始化变量及参数

```Python
# create mock up data
# count of samples
m = 100
X, Y = create_sample_data(m)
plt.plot(X, Y, "b.")
plt.show()
# initialize_data
# step for each iteration
eta = 0.01
# set w,b=0, you can set to others values to have a try
w = 0
b = 0
stop_flag = False
eps = 1e-10
step = 0
max_step = 10000
prev_loss, loss, diff_loss = 10.0, 0.0, 10.0
```

## 程序主循环

```Python
while step < max_step:
    step += 1
    for i in range(m):
        # get x and y value for one sample
        x = X[i]
        y = Y[i]
        # get z from x,y
        z = forward_calculation(w, b, x)
        # calculate gradient of w and b
        dw, db = dJwb(z, y, x)
        # update w,b
        w, b = update_weights(w, b, dw, db, eta)
        # calculate loss for this batch
        loss = check_diff(w,b,X,Y, m)
        diff_loss = abs(loss - prev_loss)
        # if diff is small enough, stop <for>
        if diff_loss < eps:
            stop_flag = True
            break
        else:
            prev_loss = loss
            
    print(step, diff_loss, w, b)
    if stop_flag:   # stop <while>
        break

print(step)
print(w,b)
```

程序运行结果如下：

```Python
......
28
1.9410202946437924 3.0379868578579416
```
一共迭代了28*100次，由于diff_loss小于1e-10，所以停止了。但是，可以看到w=1.941..，b=3.03798...，与实际值w=2, b=3还有差距。这是为什么呢？
这就是所说的局部最优解了，因为我们设置的eps=1e-10，已经非常非常小了，但还是可能出现偏差。下面我们修改一下主循环，看看效果如何：
```Python
# 使用固定循环次数，不设置其它停止条件
while step < 100:   
    step += 1
    for i in range(m):
        # get x and y value for one sample
        x = X[i]
        y = Y[i]
        # get z from x,y
        z = forward_calculation(w, b, x)
        # calculate gradient of w and b
        dw, db = dJwb(z, y, x)
        # update w,b
        w, b = update_weights(w, b, dw, db, eta)

print(step)
print(w,b)
```
结果：
```Python
100
2.0081799556718307 3.002525454111184
```
循环了100次，w/b两个值非常接近于理论值了。这就是所谓的“试错”了。


# 使用第二种方式训练 - 批量样本

```Python
# use all the samples as a batch to train, then iteration on batch

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def create_sample_data(m):
    # generate X data
    Xfile = Path("XData.npy")
    Yfile = Path("YData.npy")
    if Xfile.exists() & Yfile.exists():
        X = np.load(Xfile)
        Y = np.load(Yfile)
    else:
        X = np.random.random(m)
        # create some offset as noise to simulate real data
        noise = np.random.normal(0,0.1,X.shape)
        # genarate Y data
        W = 2
        B = 3
        Y = X * W + B + noise
        np.save("XData.npy", X)
        np.save("YData.npy", Y)

    return X, Y

def forward_calculation(w,b,X):
    Z = w * X + b
    return Z

def dJwb(Z,Y,X,m):
    p = Z - Y
    db = sum(p)/m
    q = p*X
    dw = sum(q)/m
    return dw, db

def update_weights(w, b, dw, db, eta):
    w = w - eta*dw
    b = b - eta*db
    return w,b

def check_diff(w, b, X, Y):
    Z = w * X + b
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/m
    return loss

# create mock up data
# count of samples
m = 100
X, Y = create_sample_data(m)
plt.plot(X, Y, "b.")
plt.show()

# initialize_data
# step for each iteration
eta = 0.01
# set w,b=0, you can set to others values to have a try
w = 0
b = 0
# condition 1 to stop iteration: when Q - prevQ < error
eps = 1e-10
loss = 1000
prev_loss = 1000
# condition 2 to stop iteration
max_iteration = 10000
# counter of iteration
iteration = 0

# condition 2 to stop
while iteration < max_iteration:
    # using current w,b to calculate Z
    Z = forward_calculation(w,b,X)
    # get gradient value
    dW, dB = dJwb(Z, Y, X, m)
    # update w and b
    w, b = update_weights(w, b, dW, dB, eta)
#   print(iteration,w,b)
    iteration += 1
    # condition 1 to stop
    loss = check_diff(w,b,X,Y)
    if abs(loss - prev_loss) < eps:
        break
    prev_loss = loss

print(iteration)
print(w,b)
```

## 定义针对w/b的梯度函数

因为：

$$Z = wX+b$$

$$loss = \frac{1}{2m}(Z-Y)^2$$

所以我们用loss的值作为基准，去求w对它的影响，也就是loss对w的偏导数：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{Z}}*\frac{\partial{Z}}{\partial{w}}
$$

其中：

$$
\frac{\partial{loss}}{\partial{Z}} = \frac{\partial{}}{\partial{Z}}[\frac{1}{2m}(Z-Y)^2] = \frac{1}{m}(Z-Y)
$$

而：

$$
\frac{\partial{Z}}{\partial{w}} = \frac{\partial{}}{\partial{w}}(wX+b) = X
$$

所以：

$$
\frac{\partial{loss}}{\partial{w}} = \frac{\partial{loss}}{\partial{Z}}*\frac{\partial{Z}}{\partial{w}} = \frac{1}{m}(Z-Y)X
$$
对于b来说：
$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{Z}}*\frac{\partial{Z}}{\partial{b}}
$$
而：

$$
\frac{\partial{Z}}{\partial{b}} = \frac{\partial{(wX+b)}}{\partial{b}} = 1
$$
所以：

$$
\frac{\partial{loss}}{\partial{b}} = \frac{\partial{loss}}{\partial{Z}}*\frac{\partial{Z}}{\partial{b}} = \frac{1}{m}(Z-Y)
$$
写成code：

```Python
# Z,Y,X都是数组
def dJwb(Z,Y,X,m):
    p = Z - Y
    db = sum(p)/m
    q = p*X
    dw = sum(q)/m
    return dw, db
```

程序运行结果如下：

```Python
9262
2.0079540284413393 3.004665574860875
```

训练过程迭代了9262次，loss的前后差值小于1e-10了，达到了停止条件。可以看到最后w = 2.0079, b = 3.0046, 非常接近W=2, B=3的真实值。

也可以注释掉condition 1，让迭代达到10000次，但其实结果并不会好到哪里去。

# 两种方式的比较

## 随机梯度下降
1. 每次用一个样本训练，然后立刻更新权重，训练速度快，迭代了28次就停止了
2. 可以设置一个较短的迭代次数，以便得到较好的解
3. 容易陷入局部最优

## 批量梯度下降
1. 每次用整批样本训练后，才更新一次权重，训练速度慢，迭代了9000多次
2. 特定的样本如果误差较大，不会影响整体的训练质量
3. 可以得到相对全局较优的解


孔子说：点赞是人类的美德！如果觉得有用，关闭网页前，麻烦您给点个赞！然后准备学习下一周的内容。

