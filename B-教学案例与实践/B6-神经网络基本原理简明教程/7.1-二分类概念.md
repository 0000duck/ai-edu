Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 二分类原理

## 分类函数

Sigmoid本身是激活函数，又可以当作二分类的分类函数。

### 公式

$$a(z) = \frac{1}{1 + e^{-z}}$$

### 导数

$$a^{'}(z) = a(z)(1 - a(z))$$

### 输入值域

$[-\infin, \infin]$

### 输出值域

[0,1]

### 函数图像

<img src=".\Images\7\sigmoid.png">

### 工作原理

训练时，一个样本x在经过神经网络的最后一层的矩阵运算结果作为输入z，经过Sigmoid后，输出一个[0,1]之间的预测值。我们假设这个样本的标签值为0（属于负类，另外一类是第1类属于正类），如果其预测值越接近0，就越接近标签值，那么误差越小，反向传播的力度就越小。

推理时，我们预先设定一个阈值，比如上图中的红线，我们设置阈值=0.5，则当推理结果大于0.5时，认为是正类；小于0.5时认为是负类；等于0.5时，根据情况自己定义。阈值也不一定就是0.5，也可以是0.65等等，阈值越大，准确率越高，召回率越低；阈值越小则相反。

## 损失函数

$$
J(w,b) = -{1 \over m} \sum^m_{i=0}y_iln(a_i)+(1-y_i)ln(1-a_i)
$$

## 正向计算

### 神经网络计算

$$
z_i = wx_i+b  \tag{1}
$$

### 分类计算

$$
a_i={1 \over 1 + e^{-z_i}} \tag{2}
$$

### 损失函数计算

$$
J(w,b) = -{1 \over m} \sum^m_{i=0}y_iln(a_i)+(1-y_i)ln(1-a_i) \tag{3}
$$

## 反向传播

$$
{\partial{J} \over \partial{a_i}}= -{1 \over m} \sum^m_{i=0} ({y_i \over a_i} - {1-y_i \over 1-a_i})
$$

$$
=-{1 \over m} \sum^m_{i=0}{y_i-a_i \over a_i(1-a_i)} \tag{4}
$$

$$
{\partial{a_i} \over \partial{z_i}}=a_i(1-a_i) \tag{5}
$$

所以，结合公式4,5：

$$
{\partial{J} \over \partial{z_i}}={\partial{J} \over \partial{a_i}}{\partial{a_i} \over \partial{z_i}}
$$

$$
=-{1 \over m} \sum^m_{i=0}{y_i-a_i \over a_i(1-a_i)} \cdot a_i(1-a_i)
$$

$$
=-{1 \over m} \sum^m_{i=0}(y_i-a_i)
$$

$$
={1 \over m} \sum^m_{i=0}(a_i-y_i) \tag{6}
$$

至此，我们得到了z的误差，进一步向后传播给w和b，从公式1可知：

$$
{\partial{z_i} \over \partial{w}} = x_i
$$

$$
{\partial{z_i} \over \partial{b}} = 1
$$

把公式6带过来：

$$
{\partial{J} \over \partial{w}}={\partial{J} \over \partial{z_i}}{\partial{z_i} \over \partial{w}}={1 \over m} \sum^m_{i=0}(a_i-y_i)x_i \tag{7}
$$

$$
{\partial{J} \over \partial{b}}={\partial{J} \over \partial{z_i}}{\partial{z_i} \over \partial{b}}={1 \over m} \sum^m_{i=0}(a_i-y_i) \tag{8}
$$

## 神经网络的线性二分类工作原理

木头：老师，我有个问题！如下图所示，我们假设绿色方块为正类：标签值$y=1$，红色三角形为负类：标签值$y=0$。

<img src=".\Images\7\linear_binary_analysis.png">

从直观上理解，如果我们有一条直线，其公式为：$z = w \cdot x1+b$，如图中的虚线所示，则所有正类的样本的x2都大于z，而所有的负类样本的x2都小于z，那么这条直线就是我们需要的解决方案。用正例的样本来表示：
$$
x2 > z => x2 > w \cdot x1 + b \tag{10}
$$

那么神经网络用矩阵运算+分类函数+损失函数这么复杂的流程，其工作原理是什么呢？

铁柱：好问题！机器学习中的SVM确实就是用这种思路来解决这个问题的，神经网络的正向公式是这样：

$$
Z = w1 \cdot x1 + w2 \cdot x2 + B
$$
$$
A = sigmoid(Z) = {1 \over 1 + e^{-Z}}
$$

<img src=".\Images\7\sigmoid_binary.png">

当A>0.5时，y=1，为正类。当A<0.5时，y=0，为负类。我们用正类来举例子：
$$
A = sigmoid(Z) = {1 \over 1 + e^{-Z}} > 0.5
$$
做公式变形，再两边取自然对数，可以得到：
$$
Z > 0
$$
即：
$$
Z = w1 \cdot x1 + w2 \cdot x2 + B > 0
$$
对上式做一下变形：
$$
x2 > - {w1 \over w2}x1 - {B \over w2} \tag{11}
$$
简化一下两个系数：
$$
x2 > w'x1 + b' \tag{12}
$$
比较一下公式10和12，
$$
x2 > w \cdot x1 + b \tag{10}
$$
$$
x2 > w'x1 + b' \tag{12}
$$
一摸一样！这就说明神经网络的数学原理和我们在二维平面上的直观是相同的。由此，我们还得到了一个额外的收获，即：
$$
w = - w1 / w2
$$
$$
b = -B/w2
$$
我们可以使用神经网络计算出w1,w2,B以后，换算成w,b来在二维平面上画那根直线，来直观地判断神经网络的正确性。
